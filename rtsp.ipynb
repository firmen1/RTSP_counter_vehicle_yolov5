{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "#Initialise the object tracker class\n",
    "object_tracker = DeepSort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class for object detection which plots boxes and scores frames in addition to detecting an\n",
    "# object\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "class YoloDetector():\n",
    "\n",
    "    def __init__(self, model):\n",
    "        #Using yolov5s for our purposes of object detection, you may use a larger model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path=model, force_reload= True)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Using Device: ', self.device)\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        downscale_factor = 2\n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "        results = self.model(frame)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "\n",
    "        return labels, cord\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        return self.classes[int(x)]\n",
    "\n",
    "    def plot_boxes(self, results, frame, height, width, confidence=0.15):\n",
    "\n",
    "      labels, cord = results\n",
    "      detections = []\n",
    "\n",
    "      n = len(labels)\n",
    "      x_shape, y_shape = width, height\n",
    "\n",
    "      for i in range(n):\n",
    "        row = cord[i]\n",
    "        if row[4]>=confidence:\n",
    "            x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)\n",
    "            detections.append(([x1, y1, int(x2-x1), int(y2-y1)], row[4].item(), str(self.class_to_label(labels[i]))))\n",
    "      return frame, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "import onnxruntime\n",
    "import torch\n",
    "class PreprocessingTracker():\n",
    "\n",
    "    def __init__(self):\n",
    "        #Using yolov5s for our purposes of object detection, you may use a larger model\n",
    "        # self.onnx_model_path = '/home/firmen@student.ub.ac.id/Skripsi_Object_Tracking/uji_ablasi/model640.onnx'\n",
    "        self.onnx_model_path = 'C:/Users/acer/Documents/coolyeah/skripsi/Revisi/real time code/model640.onnx'\n",
    "        self.onnx_session = onnxruntime.InferenceSession(self.onnx_model_path)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Using Device: ', self.device)\n",
    "    def learning_preprocessing(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
    "        \n",
    "        resized_image = cv2.resize(img, (640, 640)).astype(np.float32) /255.0\n",
    "        input_data = np.transpose(resized_image, (2, 0, 1))\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        output = self.onnx_session.run(None, {'input': input_data})\n",
    "        prediction = np.squeeze(output[0])\n",
    "\n",
    "        pred_mask = prediction\n",
    "        pred_mask[pred_mask < 0] = 0\n",
    "        pred_mask[pred_mask > 0] = 1\n",
    "        pred_mask = np.uint8(pred_mask * 255)\n",
    "        resized_image = cv2.resize(img, (1920, 1080))\n",
    "        resized_mask = cv2.resize(pred_mask, (1920, 1080))\n",
    "        inpainted_image = cv2.inpaint(resized_image, resized_mask, 3, cv2.INPAINT_TELEA)\n",
    "        fix_image = cv2.cvtColor(inpainted_image, cv2.COLOR_BGR2RGB)\n",
    "        return fix_image\n",
    "\n",
    "    def CV2_to_PIL_img(self, cv2_im):\n",
    "        cv2_im = cv2.cvtColor(cv2_im, cv2.COLOR_BGR2RGB)\n",
    "        pil_im = Image.fromarray(cv2_im)\n",
    "        return pil_im\n",
    "\n",
    "    def PIL_to_CV2_img(self, img):\n",
    "        cv_image = np.array(img.convert('RGB')) \n",
    "        cv_image = cv_image[:, :, ::-1].copy() \n",
    "        return cv_image\n",
    "\n",
    "    def first_polynomial_function(self, image):\n",
    "        table = np.array([1.657766*i-0.009157128*(i**2) + 0.00002579473*(i**3)\n",
    "            for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        return cv2.LUT(image, table)\n",
    "\n",
    "    def second_polynomial_function(self, image):\n",
    "        table = np.array([\n",
    "            -4.263256 * math.exp(-14)+1.546429*i-0.005558036*(i**2)+0.00001339286*(i**3)\n",
    "            for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        return cv2.LUT(image, table)\n",
    "\n",
    "    def adjust_gamma(self, image, gamma=1.0):\n",
    "        invGamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "            for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "                \n",
    "        return cv2.LUT(image, table)\n",
    "\n",
    "    def enhance_contrast(self, image, factor=1.4):\n",
    "        _image = ImageEnhance.Contrast(\n",
    "            self.CV2_to_PIL_img(image)\n",
    "        ).enhance(factor)\n",
    "        \n",
    "        return self.PIL_to_CV2_img(_image)\n",
    "\n",
    "    def reduce_glare(self, image):\n",
    "        _image = self.adjust_gamma(\n",
    "            self.second_polynomial_function(\n",
    "                self.adjust_gamma(\n",
    "                    self.first_polynomial_function(image), \n",
    "                    0.75\n",
    "                )\n",
    "            ),\n",
    "            0.8\n",
    "        )\n",
    "        return _image\n",
    "\n",
    "    def mix_filter(self, image):\n",
    "        _image = self.enhance_contrast(\n",
    "            self.reduce_glare(\n",
    "                self.enhance_contrast(\n",
    "                    self.reduce_glare(image), \n",
    "                    factor=1.6\n",
    "                )\n",
    "            ), \n",
    "            factor=1.4\n",
    "        )\n",
    "        return _image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\acer/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-4-1 Python-3.9.2 torch-2.1.1+cpu CPU\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:50\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:467\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax()), \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pathlib.py:1073\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m-> 1073\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:65\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# arbitrary model\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pathlib.py:1073\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m-> 1073\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrtsp://10.45.188.254:31554/nvstream/home/vst/vst_release/streamer_videos/veteran_in_20230522T111503.mkv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m object_tracker \u001b[38;5;241m=\u001b[39m DeepSort()\n\u001b[1;32m----> 4\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mYoloDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/acer/Documents/coolyeah/skripsi/Revisi/real time code/no_preprocessing.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m preprocessing_frame \u001b[38;5;241m=\u001b[39m PreprocessingTracker()\n\u001b[0;32m      6\u001b[0m set_id_datang \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mYoloDetector.__init__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#Using yolov5s for our purposes of object detection, you may use a larger model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnames\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    563\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    564\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 566\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\hub.py:595\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    594\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> 595\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:88\u001b[0m, in \u001b[0;36mcustom\u001b[1;34m(path, autoshape, _verbose, device)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:83\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     81\u001b[0m help_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for help.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"rtsp://10.45.188.254:31554/nvstream/home/vst/vst_release/streamer_videos/veteran_in_20230522T111503.mkv\")\n",
    "object_tracker = DeepSort()\n",
    "\n",
    "detector = YoloDetector('C:/Users/acer/Documents/coolyeah/skripsi/Revisi/real time code/no_preprocessing.pt')\n",
    "preprocessing_frame = PreprocessingTracker()\n",
    "set_id_datang = set()\n",
    "set_id_lewat = set()\n",
    "# Define the counting line\n",
    "line_start = (0, 600)\n",
    "line_end = (1700, 600)\n",
    "\n",
    "# Initialize counters\n",
    "counter_car = 0\n",
    "counter_motorbike = 0\n",
    "\n",
    "while True or cap.isOpened():\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    # if preprocessing_type == 'machine_learning':\n",
    "    #         preprocessing_img = preprocessing_frame.learning_preprocessing(img)\n",
    "    # elif preprocessing_type == 'reduce_glare':\n",
    "    #     preprocessing_img = preprocessing_frame.mix_filter(img)\n",
    "    #     best_confidence = 0.421\n",
    "    # else:\n",
    "    #     preprocessing_img = img\n",
    "    #     best_confidence = 0.566\n",
    "    preprocessing_img = img\n",
    "    best_confidence = 0.566\n",
    "\n",
    "    results = detector.score_frame(preprocessing_img)\n",
    "    img, detections = detector.plot_boxes(results, preprocessing_img, height=preprocessing_img.shape[0], width=preprocessing_img.shape[1], confidence=best_confidence)\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "\n",
    "        bbox = ltrb\n",
    "        # c_x, c_y, _, _ =  track.to_tlbr()\n",
    "\n",
    "        # Draw the counting line\n",
    "        cv2.line(img, line_start, line_end, (255, 250, 250), 2)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw track ID and class label\n",
    "        cv2.putText(img, f\"ID: {str(track_id)} {track.get_det_class()}\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Check if the bounding box crosses the counting line\n",
    "        if bbox[1] < line_start[1] and bbox[3] > line_start[1]:\n",
    "            if track.get_det_class() == 'Car' and track_id not in set_id_lewat and track_id in set_id_datang:\n",
    "                counter_car += 1\n",
    "                set_id_lewat.add(track_id)\n",
    "            elif track.get_det_class() == 'Motorbike' and track_id not in set_id_lewat and track_id in set_id_datang:\n",
    "                counter_motorbike += 1\n",
    "                set_id_lewat.add(track_id)\n",
    "        else:\n",
    "            set_id_datang.add(track_id) if track_id not in set_id_datang else None\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20, 1080 - 170), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display counters\n",
    "    cv2.putText(img, f'Car Count: {counter_car}', (20, 1080 - 120), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "    cv2.putText(img, f'Motorbike Count: {counter_motorbike}', (20, 1080 - 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Object Counting\", img)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# Define video writer parameters\n",
    "def make_track_video(src_video_path, dest_path, model, preprocessing_type):\n",
    "    object_tracker = DeepSort()\n",
    "\n",
    "    detector = YoloDetector(model)\n",
    "    preprocessing_frame = PreprocessingTracker()\n",
    "\n",
    "    cap = cv2.VideoCapture(src_video_path)\n",
    "\n",
    "    # Setting resolution for webcam\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 640)\n",
    "    output_path = dest_path\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'XVID' or 'MJPG' for different video codecs\n",
    "    fps = 20  # Adjust the frames per second as needed\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (1920, 1080))  # Make sure to set 'width' and 'height' to the appropriate values\n",
    "\n",
    "    # Define the counting line\n",
    "    line_start = (0, 600)\n",
    "    line_end = (1700, 600)\n",
    "\n",
    "    # Initialize counters\n",
    "    counter_car = 0\n",
    "    counter_motorbike = 0\n",
    "    best_confidence = 0.3\n",
    "\n",
    "    set_id_datang = set()\n",
    "    set_id_lewat = set()\n",
    "    while cap.isOpened():\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        # print(img.shape)\n",
    "        start = time.perf_counter()\n",
    "        if preprocessing_type == 'machine_learning':\n",
    "            preprocessing_img = preprocessing_frame.learning_preprocessing(img)\n",
    "        elif preprocessing_type == 'reduce_glare':\n",
    "            preprocessing_img = preprocessing_frame.mix_filter(img)\n",
    "            best_confidence = 0.421\n",
    "        else:\n",
    "            preprocessing_img = img\n",
    "            best_confidence = 0.566\n",
    "\n",
    "        results = detector.score_frame(preprocessing_img)\n",
    "        img, detections = detector.plot_boxes(results, preprocessing_img, height=preprocessing_img.shape[0], width=preprocessing_img.shape[1], confidence=best_confidence)\n",
    "        tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "\n",
    "            bbox = ltrb\n",
    "            # c_x, c_y, _, _ =  track.to_tlbr()\n",
    "\n",
    "            # Draw the counting line\n",
    "            cv2.line(img, line_start, line_end, (255, 250, 250), 2)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "\n",
    "            # Draw track ID and class label\n",
    "            cv2.putText(img, f\"ID: {str(track_id)} {track.get_det_class()}\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Check if the bounding box crosses the counting line\n",
    "            if bbox[1] < line_start[1] and bbox[3] > line_start[1]:\n",
    "                if track.get_det_class() == 'Car' and track_id not in set_id_lewat and track_id in set_id_datang:\n",
    "                    counter_car += 1\n",
    "                    set_id_lewat.add(track_id)\n",
    "                elif track.get_det_class() == 'Motorbike' and track_id not in set_id_lewat and track_id in set_id_datang:\n",
    "                    counter_motorbike += 1\n",
    "                    set_id_lewat.add(track_id)\n",
    "            else:\n",
    "                set_id_datang.add(track_id) if track_id not in set_id_datang else None\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        totalTime = end - start\n",
    "        fps = 1 / totalTime\n",
    "\n",
    "        cv2.putText(img, f'FPS: {int(fps)}', (20, 1080 - 170), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display counters\n",
    "        cv2.putText(img, f'Car Count: {counter_car}', (20, 1080 - 120), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "        cv2.putText(img, f'Motorbike Count: {counter_motorbike}', (20, 1080 - 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame to the video\n",
    "        video_writer.write(img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# cap = cv2.VideoCapture(\"rtsp://10.45.188.254:31554/nvstream/home/vst/vst_release/streamer_videos/veteran_in_20230522T111503.mkv\")\n",
    "\n",
    "# while True:\n",
    "#     _, img = cap.read()\n",
    "#     cv2.imshow(\"Object Counting\", img)\n",
    "#     key = cv2.waitKey(1) & 0xFF\n",
    "#     if key == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
